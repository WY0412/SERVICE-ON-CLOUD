{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val df = spark.read.json(\"gs://cmuccpublicdatasets/twitter/dataset/\").cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val df1 = spark.read.json(\"gs://cmuccpublicdatasets/twitter/dataset/part-r-00000.gz\").cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import java.nio.file.{Files, Paths}\n",
        "import scala.jdk.CollectionConverters._\n",
        "\n",
        "val dirPath = \"gs://cmuccpublicdatasets/twitter/dataset/\"\n",
        "\n",
        "val fileList = Files.list(Paths.get(dirPath)).iterator().asScala\n",
        "for (path <- fileList) {\n",
        "  println(path.getFileName)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "// Assuming you've already initialized SparkSession as 'spark'\n",
        "val basePath = \"gs://cmuccpublicdatasets/twitter/dataset/\"\n",
        "\n",
        "// Generate file paths\n",
        "val fileNames = (0 until 50).map { i =>\n",
        "  f\"$basePath%spart-r-${i}%05d.gz\"\n",
        "}\n",
        "\n",
        "// Read the first file to initialize df2\n",
        "var df2 = spark.read.json(fileNames.head).cache()\n",
        "\n",
        "// Read the remaining files and union them with df2\n",
        "fileNames.tail.foreach { filePath =>\n",
        "  val nextDf = spark.read.json(filePath)\n",
        "  df2 = df2.unionByName(nextDf, allowMissingColumns = true)\n",
        "}\n",
        "df2 = df2.cache\n",
        "// Now, df2 contains the data from the first 50 files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "df2.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "basePath = \"gs://cmuccpublicdatasets/twitter/dataset/\"\n",
        "fileNames = [f\"{basePath}part-r-{str(i).zfill(5)}.gz\" for i in range(50)]\n",
        "print(fileNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Multiple JSON Files\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "# Assuming SparkSession has already been initialized as 'spark'\n",
        "basePath = \"gs://cmuccpublicdatasets/twitter/dataset/\"\n",
        "\n",
        "basePath = \"gs://cmuccpublicdatasets/twitter/dataset/\"\n",
        "fileNames = [f\"{basePath}part-r-{str(i).zfill(5)}.gz\" for i in range(50)]\n",
        "print(fileNames)\n",
        "# Read all files at once\n",
        "df2 = spark.read.json(fileNames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "df2.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "from pyspark.sql.functions import col, concat_ws, size\n",
        "\n",
        "valid_tweets = df2.filter(\n",
        "    (col(\"id\").isNotNull() | col(\"id_str\").isNotNull()) &\n",
        "    (col(\"user.id\").isNotNull() | col(\"user.id_str\").isNotNull()) &\n",
        "    col(\"created_at\").isNotNull() &\n",
        "    (col(\"text\").isNotNull() & (col(\"text\") != \"\")) &\n",
        "    (col(\"entities.hashtags\").isNotNull() & (size(col(\"entities.hashtags\")) > 0)) &\n",
        "    col(\"lang\").isin(\"ar\", \"en\", \"fr\", \"in\", \"pt\", \"es\", \"tr\", \"ja\")\n",
        ").dropDuplicates([\"id\"])\n",
        "\n",
        "new_df = valid_tweets.withColumn(\"hashtags\", concat_ws(\", \", col(\"entities.hashtags.text\"))).select(\n",
        "    col(\"created_at\"),\n",
        "    col(\"hashtags\"),\n",
        "    col(\"id\"),\n",
        "    col(\"in_reply_to_user_id\").alias(\"reply_uid\"),\n",
        "    col(\"retweeted_status.id\").alias(\"retweet_tid\"),\n",
        "    col(\"retweeted_status.user.id\").alias(\"retweet_uid\"),\n",
        "    col(\"text\"),\n",
        "    col(\"user.id\").alias(\"uid\"),\n",
        "    col(\"user.screen_name\").alias(\"user_screen_name\"),\n",
        "    col(\"user.description\").alias(\"user_description\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "import pandas as pd\n",
        "\n",
        "# Set credentials\n",
        "jdbc_port = \"3306\"\n",
        "jdbc_database = \"etl_db\"\n",
        "jdbc_username = \"etl_user\"\n",
        "jdbc_password = \"PAYTON401gzt\"  # The password you set\n",
        "jdbc_hostname = \"34.139.20.62\"  # The external IP address of your MySQL DB\n",
        "\n",
        "jdbc_url = f\"jdbc:mysql://{jdbc_hostname}:{jdbc_port}/{jdbc_database}?user={jdbc_username}&password={jdbc_password}\"\n",
        "\n",
        "\n",
        "# Set connection properties\n",
        "properties = {\n",
        "    \"user\": jdbc_username,\n",
        "    \"password\": jdbc_password,\n",
        "    \"driver\": \"com.mysql.jdbc.Driver\",\n",
        "    \"useServerPrepStmts\": \"false\",  # Note this configuration\n",
        "    \"rewriteBatchedStatements\": \"true\",  # Note this configuration\n",
        "}\n",
        "\n",
        "new_df.write.jdbc(url=jdbc_url, table=\"tweets\", mode=\"overwrite\", properties=properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "df2.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "df1.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val sample_df = df.limit(100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val validTweets = df3.filter(\n",
        "  (col(\"id\").isNotNull || col(\"id_str\").isNotNull) &&\n",
        "  (col(\"user.id\").isNotNull || col(\"user.id_str\").isNotNull) &&\n",
        "  col(\"created_at\").isNotNull &&\n",
        "  (col(\"text\").isNotNull && col(\"text\") =!= \"\") &&\n",
        "  (col(\"entities.hashtags\").isNotNull && size(col(\"entities.hashtags\")) > 0) &&\n",
        "  col(\"lang\").isin(\"ar\", \"en\", \"fr\", \"in\", \"pt\", \"es\", \"tr\", \"ja\")\n",
        ")\n",
        ".dropDuplicates(\"id\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val newDf = validTweets\n",
        ".withColumn(\"hashtags\", concat_ws(\", \", col(\"entities.hashtags.text\")))\n",
        ".select(\n",
        "    col(\"created_at\"),\n",
        "    col(\"hashtags\"),\n",
        "    col(\"id\"),\n",
        "    col(\"in_reply_to_user_id\").as(\"reply_uid\"),\n",
        "    col(\"retweeted_status.id\").as(\"retweet_tid\"),\n",
        "    col(\"retweeted_status.user.id\").as(\"retweet_uid\"),\n",
        "    col(\"text\"),\n",
        "    col(\"user.id\").as(\"uid\"),\n",
        "    col(\"user.screen_name\").as(\"user_screen_name\"),\n",
        "    col(\"user.description\").as(\"user_description\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import java.util.Properties\n",
        "import java.sql.DriverManager\n",
        "\n",
        "// set credentials\n",
        "val jdbcPort = \"3306\"\n",
        "val jdbcDatabase = \"etl_db\"\n",
        "val jdbcUsername = \"etl_user\"\n",
        "val jdbcPassword = \"PAYTON401gzt\"   // the password you set\n",
        "val jdbcHostname = \"34.139.20.62\"   // the external IP address of you MySQL DB\n",
        "\n",
        "val jdbcUrl =s\"jdbc:mysql://$jdbcHostname:$jdbcPort/$jdbcDatabase\"\n",
        "\n",
        "val driverClass = \"com.mysql.jdbc.Driver\"\n",
        "Class.forName(driverClass)  // check jdbc driver\n",
        "\n",
        "// set connection properties\n",
        "val connectionProperties = new Properties()\n",
        "connectionProperties.put(\"user\", s\"$jdbcUsername\")\n",
        "connectionProperties.put(\"password\", s\"$jdbcPassword\")\n",
        "connectionProperties.setProperty(\"Driver\", driverClass)\n",
        "connectionProperties.setProperty(\"useServerPrepStmts\", \"false\")    // take note of this configuration, and understand what it does\n",
        "connectionProperties.setProperty(\"rewriteBatchedStatements\", \"true\")  // take note of this configuration, and understand what it does\n",
        "\n",
        "// first drop the table if it already exists\n",
        "val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)\n",
        "assert(!connection.isClosed)\n",
        "val stmt = connection.createStatement()\n",
        "stmt.executeUpdate(\"drop table if exists tweets\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// write the dataframe to a table called \"biz_review\"\n",
        "newDf.write.jdbc(jdbcUrl, \"tweets\", connectionProperties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "mysql -h34.139.20.62 -pPAYTON401gzt -uetl_user -e 'use etl_db; select * from tweets limit 1;'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Official Write Trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val validTweets = sample_df.filter(\n",
        "  (col(\"id\").isNotNull || col(\"id_str\").isNotNull) &&\n",
        "  (col(\"user.id\").isNotNull || col(\"user.id_str\").isNotNull) &&\n",
        "  col(\"created_at\").isNotNull &&\n",
        "  (col(\"text\").isNotNull && col(\"text\") =!= \"\") &&\n",
        "  (col(\"entities.hashtags\").isNotNull && size(col(\"entities.hashtags\")) > 0) &&\n",
        "  col(\"lang\").isin(\"ar\", \"en\", \"fr\", \"in\", \"pt\", \"es\", \"tr\", \"ja\")\n",
        ")\n",
        ".dropDuplicates(\"id_str\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val newDf_all = validTweets\n",
        ".withColumn(\"hashtags\", concat_ws(\", \", col(\"entities.hashtags.text\")))\n",
        ".select(\n",
        "    col(\"created_at\"),\n",
        "    col(\"hashtags\"),\n",
        "    col(\"id\"),\n",
        "    col(\"in_reply_to_user_id\").as(\"reply_uid\"),\n",
        "    col(\"retweeted_status.id\").as(\"retweet_tid\"),\n",
        "    col(\"retweeted_status.user.id\").as(\"retweet_uid\"),\n",
        "    col(\"text\"),\n",
        "    col(\"user.id\").as(\"uid\"),\n",
        "    col(\"user.screen_name\").as(\"user_screen_name\"),\n",
        "    col(\"user.description\").as(\"user_description\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import java.util.Properties\n",
        "import java.sql.DriverManager\n",
        "\n",
        "// set credentials\n",
        "val jdbcPort = \"3306\"\n",
        "val jdbcDatabase = \"etl_db\"\n",
        "val jdbcUsername = \"etl_user\"\n",
        "val jdbcPassword = \"PAYTON401gzt\"   // the password you set\n",
        "val jdbcHostname = \"34.139.20.62\"   // the external IP address of you MySQL DB\n",
        "\n",
        "val jdbcUrl =s\"jdbc:mysql://$jdbcHostname:$jdbcPort/$jdbcDatabase\"\n",
        "\n",
        "val driverClass = \"com.mysql.jdbc.Driver\"\n",
        "Class.forName(driverClass)  // check jdbc driver\n",
        "\n",
        "// set connection properties\n",
        "val connectionProperties = new Properties()\n",
        "connectionProperties.put(\"user\", s\"$jdbcUsername\")\n",
        "connectionProperties.put(\"password\", s\"$jdbcPassword\")\n",
        "connectionProperties.setProperty(\"Driver\", driverClass)\n",
        "connectionProperties.setProperty(\"useServerPrepStmts\", \"false\")    // take note of this configuration, and understand what it does\n",
        "connectionProperties.setProperty(\"rewriteBatchedStatements\", \"true\")  // take note of this configuration, and understand what it does\n",
        "\n",
        "// first drop the table if it already exists\n",
        "val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)\n",
        "assert(!connection.isClosed)\n",
        "val stmt = connection.createStatement()\n",
        "stmt.executeUpdate(\"drop table if exists tweets\")\n",
        "\n",
        "// write the dataframe to a table called \"biz_review\"\n",
        "newDf_all.write.jdbc(jdbcUrl, \"tweets\", connectionProperties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val df1 = spark.read.json(\"gs://cmuccpublicdatasets/twitter/dataset/part-r-00000.gz\").cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val correct_df1 = spark.read.json(\"microservice3_ref.txt\").cache "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct_df1.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "val filtered_tweets_df = df1.filter(\n",
        "  (col(\"id\").isNotNull || col(\"id_str\").isNotNull) &&\n",
        "  (col(\"user.id\").isNotNull || col(\"user.id_str\").isNotNull) &&\n",
        "  col(\"created_at\").isNotNull &&\n",
        "  (col(\"text\").isNotNull && col(\"text\") =!= \"\") &&\n",
        "  (col(\"entities.hashtags\").isNotNull && size(col(\"entities.hashtags\")) > 0) &&\n",
        "  col(\"lang\").isin(\"ar\", \"en\", \"fr\", \"in\", \"pt\", \"es\", \"tr\", \"ja\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_tweets_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "val hashtagCounts = df\n",
        "  .withColumn(\"hashtag\", explode($\"hashtags\")) \n",
        "  .groupBy(\"hashtag\")\n",
        "  .count() \n",
        "  .withColumn(\"logCount\", log10($\"count\")) \n",
        "  .orderBy($\"count\".desc) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val hashtagFrequencies = hashtagCounts.write.csv(\"hashtag_frequencies.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val xValues = hashtagFrequencies.map(_.getString(0)) \n",
        "val yValues = hashtagFrequencies.map(_.getDouble(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%python\n",
        "hashtag = pd.read_csv('hashtag_frequencies.csv')\n",
        "df_sorted = hashtag.sort_values(by='count', ascending=False)\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# We use log=True to indicate we want the y-axis to be on a log scale.\n",
        "# If you want to plot the logCount directly, ensure it's correctly calculated.\n",
        "plt.hist(df_sorted['logCount'], bins=50, log=True)\n",
        "\n",
        "plt.title('Histogram of Hashtag Frequencies on a Log Scale')\n",
        "plt.xlabel('Log of Hashtag Frequency')\n",
        "plt.ylabel('Number of Hashtags')\n",
        "plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "ETL trial 3"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
